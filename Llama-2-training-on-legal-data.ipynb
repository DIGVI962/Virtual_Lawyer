{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from llama-cpp-python) (4.8.0)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from llama-cpp-python) (1.26.1)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-win_amd64.whl size=1073112 sha256=f5ba1a9131069377bcaa212dba87f0426c68ca836fb5d5adeae877ae614fb9fd\n",
      "  Stored in directory: c:\\users\\iamthewizard\\appdata\\local\\pip\\cache\\wheels\\dc\\42\\77\\a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.10.22-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ------------------------------------- 981.5/981.5 kB 20.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from unstructured) (1.26.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from unstructured) (3.8.1)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.9.3-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "     ---------------------------------------- 199.4/199.4 kB ? eta 0:00:00\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "     ---------------------------------------- 358.9/358.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from unstructured) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting python-iso639\n",
      "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
      "     ---------------------------------------- 275.1/275.1 kB ? eta 0:00:00\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting python-magic\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.4.0-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from unstructured) (0.6.1)\n",
      "Collecting filetype\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from dataclasses-json->unstructured) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from nltk->unstructured) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from nltk->unstructured) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from nltk->unstructured) (4.66.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from requests->unstructured) (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from requests->unstructured) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from requests->unstructured) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\iamthewizard\\documents\\github\\stable-diffusion-webui\\extensions\\llama2training\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Using legacy 'setup.py install' for langdetect, since package 'wheel' is not installed.\n",
      "Installing collected packages: filetype, tabulate, rapidfuzz, python-magic, python-iso639, lxml, langdetect, emoji, chardet, backoff, unstructured\n",
      "  Running setup.py install for langdetect: started\n",
      "  Running setup.py install for langdetect: finished with status 'done'\n",
      "Successfully installed backoff-2.2.1 chardet-5.2.0 emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 lxml-4.9.3 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.4.0 tabulate-0.9.0 unstructured-0.10.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Create a Database of information gathered from local text files<b>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what Documents to load\n",
    "\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "loader = DirectoryLoader('./', glob = 'COI_txt_trial.txt', loader_cls = TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "\n",
    "'''\n",
    "file_path='./150_lawergpt_dataset_qna_v1_train.jsonl'\n",
    "documents = json.loads(Path(file_path).read_text())\n",
    "'''\n",
    "'''\n",
    "loader = JSONLoader(\n",
    "    file_path='./150_lawergpt_dataset_qna_v1_train.jsonl',\n",
    "    jq_schema='.content',\n",
    "    text_content=False,\n",
    "    json_lines=True)\n",
    "'''\n",
    "\n",
    "'''\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "folder_path = \"./150_lawergpt_dataset_qna_v1_train.jsonl\"\n",
    "directory_loader = DirectoryLoader(folder_path)\n",
    "documents = directory_loader.load()\n",
    "'''\n",
    "# Interpret information in the Documents\n",
    "\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "texts = splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2', model_kwargs = {'device':'cpu'})\n",
    "\n",
    "# Cretae and save the local Database\n",
    "\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local('faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COI_txt_trial.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sources = [doc.metadata['source']  for doc in documents]\n",
    "doc_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Read the database of information from local text files and uses a large language model to answer questions about their content.<b>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Preparing the template\n",
    "\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# Load the Language Model\n",
    "\n",
    "local_llm_model_name = 'llama-2-13b-chat.Q5_K_M.gguf'\n",
    "#llm = CTransformers(model = './llama-2-13b-chat.Q5_K_M.gguf', model_type = 'llama', config = {'max_new_tokens':256, 'temperature':0.01})\n",
    "llm = LlamaCpp(\n",
    "    model_path = f\"./{local_llm_model_name}\",\n",
    "    n_gpu_layers = 20,\n",
    "    n_batch = 512,\n",
    "    n_ctx = 2048,\n",
    "    f16_kv = True,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "# Load the interpreted information from local Database\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs = {'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)\n",
    "\n",
    "\n",
    "# Prepeare a version of the llm pre-loaded with the local content\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs = {'k': 2})\n",
    "prompt = PromptTemplate(template = template, input_variables = ['context', 'question'])\n",
    "qa_llm = RetrievalQA.from_chain_type(llm = llm, chain_type = 'stuff', retriever = retriever, return_source_documents = True, chain_type_kwargs = {'prompt': prompt})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Query<b>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model, question):\n",
    "    time_start = time.time()\n",
    "    question = 'Answer like a legal consultant to this query and expand upon the laws involved - ' + question\n",
    "    output = model({'query': question})\n",
    "    response = output[\"result\"]\n",
    "    time_elapsed = time.time() - time_start\n",
    "    display(HTML(f'<code>{local_llm_model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
    "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
    "    display(HTML(f'<strong>Answer:</strong> {response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<code>llama-2-13b-chat.Q5_K_M.gguf response time: 255.95 sec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> Answer like a legal consultant to this query and expand upon the laws involved - what punishment will I face if I commit arson, and how long is the jail time and fine in indian rupees?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> If you have committed arson, then you are guilty of an offense known as 'criminal intimidation' under section 506 of the Indian Penal Code, which carries a punishment of imprisonment for up to two years and a fine. In addition, if your actions caused damage to property or put human life in danger, you may also be charged with offenses such as 'damage to property' under Section 427 of the IPC and/or 'attempt to commit culpable homicide not amounting to murder' under Section 308 of the IPC. The punishment for these offenses will depend on the severity of the damage caused and the risk posed to human life.\n",
       "If you are found guilty of committing arson during a period of emergency, then the Special Courts set up under the Conservation of Foreign Exchange and Prevention of Smuggling Activities Act (COFEPOSA) may have jurisdiction over your case. In such cases, the punishment may be more severe, with imprisonment for up to five years and a fine.\n",
       "In terms of the specific punishment in Indian rupees, the fine for arson"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(qa_llm, 'what punishment will I face if I commit arson, and how long is the jail time and fine in indian rupees?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<code>llama-2-13b-chat.Q5_K_M.gguf response time: 161.89 sec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> Answer like a legal consultant to this query and expand upon the laws involved - what punishment will I face if I commit tax fraud, and how long is the jail time and fine in indian rupees?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> The relevant law for tax fraud is the Indian Penal Code (IPC) section 420 which states that if a person cheats and dishonestly induces the commission of an offence which is punishable with imprisonment, then such a person shall be liable to be punished with imprisonment of either description for a term which may extend to seven years and shall also be liable to fine.\n",
       "In addition to this, you may also face penalties under the Income Tax Act, 1961, such as fines, interest on tax evaded, and prosecution under section 276 of the IT Act. The punishment for tax fraud can range from a minimum fine of Rs 10,000 to a maximum fine of Rs 30,000 and imprisonment ranging from three years to seven years."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(qa_llm, 'what punishment will I face if I commit tax fraud, and how long is the jail time and fine in indian rupees?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Knowledge Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant that helps to generate text to form nice and human understandable answers based.\n",
    "The latest prompt contains the information, and you need to generate a human readable response based on the given information.\n",
    "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "Do not add any additional information that is not explicitly provided in the latest prompt.\n",
    "I repeat, do not add any information that is not explicitly given.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(question, context):\n",
    "   return f\"\"\"\n",
    "   The question is {question}\n",
    "   Answer the question by using the provided information:\n",
    "   {context}\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=3):\n",
    "  data = run_query(\"\"\"\n",
    "    // retrieve the embedding of the question\n",
    "    CALL apoc.ml.openai.embedding([$question], $apiKey) YIELD embedding\n",
    "    // match relevant movies\n",
    "    MATCH (m:Movie)\n",
    "    WITH m, gds.similarity.cosine(embedding, m.embedding) AS score\n",
    "    ORDER BY score DESC\n",
    "    // limit the number of relevant documents\n",
    "    LIMIT toInteger($k)\n",
    "    // retrieve graph context\n",
    "    MATCH (m)--()--(m1:Movie)\n",
    "    WITH m,m1, count(*) AS count\n",
    "    ORDER BY count DESC\n",
    "    WITH m, apoc.text.join(collect(m1.title)[..3], \", \") AS similarMovies\n",
    "    MATCH (m)-[r:ACTED_IN|DIRECTED]-(t)\n",
    "    WITH m, similarMovies, type(r) as type, collect(t.name) as names\n",
    "    WITH m, similarMovies, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\n",
    "    WITH m, similarMovies, collect(types) as contexts\n",
    "    WITH m, \"Movie title: \"+ m.title + \" year: \"+coalesce(m.released,\"\") +\" plot: \"+ coalesce(m.tagline,\"\")+\"\\n\" +\n",
    "          reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") + \"similar movies:\" + similarMovies + \"\\n\" as context\n",
    "    RETURN context\n",
    "  \"\"\", {'question': question, 'k': k, 'apiKey': openai_api_key})\n",
    "  return data['context'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    # Retrieve context\n",
    "    context = retrieve_context(question)\n",
    "    # Print context\n",
    "    for c in context:\n",
    "        print(c)\n",
    "    # Generate answer\n",
    "    response = run_query(\n",
    "        \"\"\"\n",
    "  CALL apoc.ml.openai.chat([{role:'system', content: $system},\n",
    "                      {role: 'user', content: $user}], $apiKey) YIELD value\n",
    "  RETURN value.choices[0].message.content AS answer\n",
    "  \"\"\",\n",
    "        {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": generate_user_prompt(question, context),\n",
    "            \"apiKey\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    return response[\"answer\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama2Training",
   "language": "python",
   "name": "llama2training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
